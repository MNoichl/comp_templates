{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MNoichl/tttms_public/blob/main/procuring_arxiv_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install necessary packages\n"
      ],
      "metadata": {
        "id": "aBHOvWIqvh0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xmltodict\n",
        "!pip install flatten_dict\n",
        "!pip install boto3\n",
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "UFdVtV00S90N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load packages and link google drive"
      ],
      "metadata": {
        "id": "_-i5vNYmvqFc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgUnfmQDmk2Y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import json\n",
        "\n",
        "import re\n",
        "\n",
        "import tqdm\n",
        "\n",
        "import tarfile\n",
        "import glob\n",
        "import os\n",
        "\n",
        "from shutil import copyfile, copy, copy2\n",
        "import shutil\n",
        "import xmltodict\n",
        "\n",
        "import gzip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aXCrhsK_cD91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Arxiv-Meta-data from Kaggle"
      ],
      "metadata": {
        "id": "5qvGfU1EwiJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# censored for review before publication\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!touch ~/.kaggle/kaggle.json\n",
        "\n",
        "api_token = {\"username\":\"USERNAME\",\"key\":\"PRIVAT_KEY\"}\n",
        "\n",
        "import json\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "ZG3citHLnkRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download Cornell-University/arxiv"
      ],
      "metadata": {
        "id": "8DGtf0x8nkVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('arxiv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('arxiv_metadata')"
      ],
      "metadata": {
        "id": "htdsakYCnkZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptislgIsmk2e"
      },
      "outputs": [],
      "source": [
        "json_lines = []\n",
        "for line in tqdm.tqdm_notebook(open(r'arxiv_metadata/arxiv-metadata-oai-snapshot.json', 'r')):\n",
        "    json_lines.append(json.loads(line))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl_luVbfmk2g"
      },
      "outputs": [],
      "source": [
        "selected_years = ['2019','2020','2021']\n",
        "articles_within_years = [x for x in tqdm.tqdm_notebook(json_lines) if re.findall('\\d{4}',x['versions'][0]['created'])[0] in selected_years]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del json_lines # to conserve RAM"
      ],
      "metadata": {
        "id": "4_qGwE8R2Jo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPxEi99Xmk2h"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(articles_within_years[0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download ArXiv fulltexts from Amazon-AWS"
      ],
      "metadata": {
        "id": "-R5HOJHC5BEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf config.ini\n",
        "\n",
        "\n",
        "# censored for review before publication\n",
        "with open('config.ini', 'w') as f:\n",
        "    f.write(\"\"\"[DEFAULT]\n",
        "ACCESS_KEY = ACCESS_KEY\n",
        "SECRET_KEY =SECRET_KEY\"\"\")"
      ],
      "metadata": {
        "id": "DWKSmvDJO6ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is adapted/taken from a tutorial by Brienna Herold, https://towardsdatascience.com/how-to-bulk-access-arxiv-full-text-preprints-58026e19e8ef"
      ],
      "metadata": {
        "id": "szMa_cjM62Bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, configparser\n",
        "\n",
        "s3resource = None\n",
        "\n",
        "def setup():\n",
        "    \"\"\"Creates S3 resource & sets configs to enable download.\"\"\"\n",
        "\n",
        "    print('Connecting to Amazon S3...')\n",
        "\n",
        "    # Securely import configs from private config file\n",
        "    configs = configparser.SafeConfigParser()\n",
        "    configs.read('config.ini')\n",
        "\n",
        "    # Create S3 resource & set configs\n",
        "    global s3resource\n",
        "    s3resource = boto3.resource(\n",
        "        's3',  # the AWS resource we want to use\n",
        "        aws_access_key_id=configs['DEFAULT']['ACCESS_KEY'],\n",
        "        aws_secret_access_key=configs['DEFAULT']['SECRET_KEY'],\n",
        "        region_name='us-east-1'  # same region arxiv bucket is in\n",
        "    )\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \"\"\"Runs if script is called on command line\"\"\"\n",
        "\n",
        "    # Create S3 resource & set configs\n",
        "    setup()"
      ],
      "metadata": {
        "id": "sXVNCD4EO6ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3, configparser, os, botocore\n",
        "\n",
        "def download_file(key):\n",
        "    \"\"\"\n",
        "    Downloads given filename from source bucket to destination directory.\n",
        "    Parameters\n",
        "    ----------\n",
        "    key : str\n",
        "        Name of file to download\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure src directory exists \n",
        "    if not os.path.isdir('src'):\n",
        "        os.makedirs('src')\n",
        "\n",
        "    # Download file\n",
        "    print('\\nDownloading s3://arxiv/{} to {}...'.format(key, key))\n",
        "\n",
        "    try:\n",
        "        s3resource.meta.client.download_file(\n",
        "            Bucket='arxiv', \n",
        "            Key=key,  # name of file to download from\n",
        "            Filename=key,  # path to file to download to\n",
        "            ExtraArgs={'RequestPayer':'requester'})\n",
        "    except botocore.exceptions.ClientError as e:\n",
        "        if e.response['Error']['Code'] == \"404\":\n",
        "            print('ERROR: ' + key + \" does not exist in arxiv bucket\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \"\"\"Runs if script is called on command line\"\"\"\n",
        "\n",
        "    # Download manifest file to current directory\n",
        "    download_file('src/arXiv_src_manifest.xml')"
      ],
      "metadata": {
        "id": "WQm_DdMDO6gT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('src/arXiv_src_manifest.xml', 'r') as file:\n",
        "    data = file.read()\n",
        "xml_content = xmltodict.parse(data)\n"
      ],
      "metadata": {
        "id": "5ebJD8W1SlUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manifest_frame = pd.DataFrame(xml_content['arXivSRC']['file'])"
      ],
      "metadata": {
        "id": "IqwzEt4RO6jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rm_make(dir):\n",
        "  try:\n",
        "    shutil.rmtree(dir)\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  try:\n",
        "    !mkdir $dir\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "\n",
        "def return_dir_paths(path):\n",
        "  all_files = []\n",
        "  for path, subdirs, files in os.walk(path):\n",
        "      for name in files:\n",
        "          all_files.append(os.path.join(path, name))\n",
        "  return all_files"
      ],
      "metadata": {
        "id": "3Cfs9-5rUBfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_directory = 'drive/MyDrive/arxiv_2019_2022_tex_collection_fourth_run'"
      ],
      "metadata": {
        "id": "PZP0C4KNXlYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_query_codes = []\n",
        "for year in tqdm.tqdm_notebook(selected_years):\n",
        "  for month in tqdm.tqdm_notebook(range(1,13)):\n",
        "    all_query_codes.append(year[-2:]+str(month).zfill(2))"
      ],
      "metadata": {
        "id": "vkOcMpKvkcko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for query_code in ['2012']: # all_query_codes[3:]:\n",
        "  print('Downloading code: ', query_code)\n",
        "  this_years_queries = manifest_frame[manifest_frame[\"yymm\"] == query_code]\n",
        "  for ix, row in tqdm.tqdm_notebook(this_years_queries.iterrows()):\n",
        "    key = row[\"filename\"]\n",
        "    print('    Downloading file: ', key)\n",
        "    try:\n",
        "      rm_make(\"src\")\n",
        "      s3resource.meta.client.download_file(\n",
        "        Bucket=\"arxiv\",\n",
        "        Key=key,  # name of key to download from\n",
        "        Filename=key,  # path to file to download to\n",
        "        ExtraArgs={\"RequestPayer\": \"requester\"},\n",
        "      )\n",
        "\n",
        "      # extract the whole package to temporary directory\n",
        "      rm_make(\"temporary_files\")\n",
        "      rm_make(\"temporary_files_extracted\")\n",
        "      tar = tarfile.open(key, \"r:\")\n",
        "      tar.extractall(path=\"temporary_files\")\n",
        "      tar.close()\n",
        "\n",
        "      # loop over all extracted files, copying the .tex files to our output directory, into their own folder named after their arxiv id\n",
        "      all_files = return_dir_paths(\"temporary_files\")\n",
        "      print('    Extracting file: ', key)\n",
        "      for this_file in tqdm.tqdm_notebook(all_files):\n",
        "        this_file_path = this_file.split(\"/\")[-1].replace(\".gz\", \"\")\n",
        "        try:\n",
        "          if this_file[-2:] == \"gz\":\n",
        "            tar = tarfile.open(this_file)\n",
        "            tar.extractall(\n",
        "              path=\"temporary_files_extracted/\" + this_file_path\n",
        "            )\n",
        "            tar.close()\n",
        "            for extracted_file in return_dir_paths(\n",
        "              \"temporary_files_extracted/\" + this_file_path\n",
        "            ):\n",
        "              if extracted_file.endswith(\".tex\"):\n",
        "                try:\n",
        "                  copy(\n",
        "                    extracted_file,\n",
        "                    output_directory\n",
        "                    + \"/\"\n",
        "                    + this_file_path\n",
        "                    + \"/\"\n",
        "                    + extracted_file.split(\"/\")[-1],\n",
        "                  )\n",
        "                except IOError as io_err:\n",
        "                  os.makedirs(output_directory + \"/\" + this_file_path)\n",
        "                  copy(\n",
        "                    extracted_file,\n",
        "                    output_directory\n",
        "                    + \"/\"\n",
        "                    + this_file_path\n",
        "                    + \"/\"\n",
        "                    + extracted_file.split(\"/\")[-1],\n",
        "                  )\n",
        "\n",
        "        except (KeyboardInterrupt, SystemExit):\n",
        "          print(\"program stopped manually\")\n",
        "          raise\n",
        "        except:\n",
        "          # print('file')\n",
        "          pass\n",
        "\n",
        "    except botocore.exceptions.ClientError as e:\n",
        "      if e.response[\"Error\"][\"Code\"] == \"404\":\n",
        "        print(\"ERROR: \" + key + \" does not exist in arxiv bucket\")\n"
      ],
      "metadata": {
        "id": "QDGybEbWO6nX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "procuring_arxiv_data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}