{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MNoichl/tttms_public/blob/main/text_layout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies"
      ],
      "metadata": {
        "id": "t22xziPwvMAM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfyWKTrv_Ms2"
      },
      "outputs": [],
      "source": [
        "!pip install metaknowledge\n",
        "!pip install umap-learn\n",
        "!pip install hdbscan\n",
        "!pip install faerun\n",
        "!pip install colorgram\n",
        "!pip install colorthief\n",
        "!pip install colorutils \n",
        "!pip install cylouvain\n",
        "!pip install xmltodict\n",
        "!pip install flatten_dict\n",
        "!pip install boto3\n",
        "!pip install compress-pickle\n",
        "!pip install kaggle\n",
        "!pip install compress-pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cnNGVtl_Ms0"
      },
      "source": [
        "# Loading packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4Mm_mQU_Ms3"
      },
      "outputs": [],
      "source": [
        "\n",
        "import metaknowledge as mk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from random import randint\n",
        "import datetime\n",
        "import copy\n",
        "from sklearn.cluster import KMeans\n",
        "import shelve\n",
        "\n",
        "import umap\n",
        "from scipy import stats\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "\n",
        "from scipy.sparse import coo_matrix, vstack\n",
        "from scipy.sparse import csr_matrix\n",
        "import scipy as scipy\n",
        "\n",
        "import math\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#For Tables:\n",
        "from IPython.display import display\n",
        "from IPython.display import Latex\n",
        "pd.set_option('display.max_columns', 500)\n",
        "\n",
        "#For R (ggplot2)\n",
        "%load_ext rpy2.ipython\n",
        "\n",
        "# from sklearn.externals.joblib import Memory\n",
        "# memory = Memory(cachedir='/tmp', verbose=0)\n",
        "# @memory.cache\n",
        "\n",
        "import gc\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.cluster import adjusted_rand_score, mutual_info_score\n",
        "\n",
        "import re\n",
        "from joblib import Parallel, delayed\n",
        "import multiprocessing\n",
        "#Embedding:\n",
        "#Clustering:\n",
        "import hdbscan\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "\n",
        "import colorcet as cc\n",
        "from matplotlib import colors\n",
        "from faerun import Faerun, host\n",
        "import os\n",
        "\n",
        "\n",
        "import nltk\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk import WordPunctTokenizer\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from multiprocessing.dummy import Pool  # This is a thread-based Pool\n",
        "from multiprocessing import cpu_count\n",
        "import time\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "# import colorgram\n",
        "from colorthief import ColorThief\n",
        "from colorutils import Color,hex_to_hsv,hsv_to_hex,rgb_to_hex\n",
        "import cylouvain\n",
        "\n",
        "import pickle\n",
        "import matplotlib as mpl\n",
        "\n",
        "from compress_pickle import dump, load\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaK0C2LR_MtM"
      },
      "source": [
        "# Loading Data \n",
        "(This doesn't need to run for re-runs: Just load the whole data below)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arxiv Data"
      ],
      "metadata": {
        "id": "1vyjnMOJ3W2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete before publication !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "\n",
        "\n",
        "!mkdir ~/.kaggle\n",
        "!touch ~/.kaggle/kaggle.json\n",
        "\n",
        "api_token = {\"username\":\"kaggle_username\",\"key\":\"kaggle_private_key\"}\n",
        "\n",
        "import json\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "YwvIyFYjEo2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download Cornell-University/arxiv"
      ],
      "metadata": {
        "id": "hgVjFyiNFUA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('arxiv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('arxiv_metadata')"
      ],
      "metadata": {
        "id": "nJbhQv0xcXXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_years = ['2019','2020']\n",
        "json_lines = []\n",
        "for line in tqdm(open(r'arxiv_metadata/arxiv-metadata-oai-snapshot.json', 'r')):\n",
        "  my_dict = json.loads(line)\n",
        "  if re.findall('\\d{4}',my_dict['versions'][0]['created'])[0] in selected_years:\n",
        "    json_lines.append(my_dict)#json.loads(line))\n",
        "\n",
        "arxiv_data = pd.DataFrame(json_lines)\n",
        "del json_lines\n"
      ],
      "metadata": {
        "id": "8znakxTYEo34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BiorXiv"
      ],
      "metadata": {
        "id": "qJCTtqjMSqlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1hO6iNDVNJwevDLKQXJoqqiKZN9Y3wVCK"
      ],
      "metadata": {
        "id": "aPEFiK_silr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "biorxiv_df = pd.read_pickle(\"full_frame.pkl\")  "
      ],
      "metadata": {
        "id": "cBCTp1XnSoq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "biorxiv_df"
      ],
      "metadata": {
        "id": "vx1bkFilSoua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "biorxiv_df_redux = biorxiv_df[[\"published\",\t\"title\", \t\"authors\",\t\"date\", \"abstract\", \"category\",\"doi\"]]"
      ],
      "metadata": {
        "id": "LJ8bhAz0SoyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arxiv_data_redux = arxiv_data[[\"doi\",\t\"title\", \t\"authors\",\t\"update_date\", \"abstract\", \"categories\",\"id\"]]\n",
        "arxiv_data_redux.columns = [\"doi\",\t\"title\", \t\"authors\",\t\"date\", \"abstract\", \"category\",\"id\"]\n",
        "biorxiv_df_redux.columns = [\"doi\",\t\"title\", \t\"authors\",\t\"date\", \"abstract\", \"category\",\"id\"]\n",
        "\n",
        "biorxiv_df_redux['origin'] = 'biorxiv'\n",
        "arxiv_data_redux['origin'] = 'arxiv'\n",
        "\n",
        "full_data = pd.concat([biorxiv_df_redux,arxiv_data_redux])"
      ],
      "metadata": {
        "id": "uYifGw0JEo6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data =full_data.drop_duplicates(subset=['id'], keep='last').reset_index(drop=True)"
      ],
      "metadata": {
        "id": "4RKpiqF2fI2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load full-data from drive"
      ],
      "metadata": {
        "id": "IWSPBSaS3oFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy https://drive.google.com/file/d/1-5ZrFDj_6tff7tCRd8nqlVf6Ls-l8-x1/view?usp=sharing -O \"full_data_joined_with_formulas.bz\"\n",
        "full_data = load(\"full_data_joined_with_formulas.bz\")"
      ],
      "metadata": {
        "id": "CgTaHqwtiu1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gw9Igqd_MtU"
      },
      "source": [
        "# Prepare texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AbsssuO_MtU"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwKb9y36_MtV"
      },
      "outputs": [],
      "source": [
        "en_stop =set(stopwords.words('english'))\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(document):\n",
        "        # Remove all the special characters and numbers\n",
        "        document = re.sub(r'\\W|\\d', ' ', str(document))\n",
        "\n",
        "        # remove all single characters\n",
        "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "        # Remove single characters from the start\n",
        "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "        # Substituting multiple spaces with single space\n",
        "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "\n",
        "        # Removing prefixed 'b'\n",
        "        document = re.sub(r'^b\\s+', '', document)\n",
        "\n",
        "        # Converting to Lowercase\n",
        "        document = document.lower()\n",
        "\n",
        "        # Lemmatization\n",
        "        tokens = document.split()\n",
        "        tokens = [stemmer.lemmatize(word) for word in tokens]\n",
        "#         tokens = [word for word in tokens if word not in en_stop]\n",
        "        tokens = [word for word in tokens if len(word) > 3]\n",
        "\n",
        "        preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "        return preprocessed_text\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN6YIp_J_MtV"
      },
      "outputs": [],
      "source": [
        "df = full_data['abstract']\n",
        "pool = Pool(8)\n",
        "results = list(tqdm(pool.imap(preprocess_text, df))) # this does need to be restarted twice!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBsQoBWl_MtX"
      },
      "outputs": [],
      "source": [
        "\n",
        "vec = TfidfVectorizer(min_df=15,\n",
        "                     norm=None,\n",
        "                      use_idf=True, smooth_idf=True, sublinear_tf=True, max_df=0.4)\n",
        "bow = vec.fit_transform(results)\n",
        "SVD = TruncatedSVD(n_components= 100, n_iter=7, random_state=42)\n",
        "XSVD = SVD.fit_transform(bow)\n",
        "print(pd.DataFrame(bow.sum(axis=0),columns=vec.get_feature_names()).T[0].sort_values())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute the umap-embedding"
      ],
      "metadata": {
        "id": "ucPIOb7idYVY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceApI6rf_MtY"
      },
      "outputs": [],
      "source": [
        "import umap\n",
        "from scipy import stats\n",
        "to_cluster_umap = umap.UMAP(random_state=20,\n",
        "                    n_components=2,\n",
        "                    n_neighbors=20,\n",
        "                    min_dist=0.1,\n",
        "                    metric='cosine',\n",
        "                    verbose=True,\n",
        "                    low_memory=True)\n",
        "\n",
        "to_cluster_umap.fit(XSVD)\n",
        "to_cluster = to_cluster_umap.embedding_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster the k-nn-graph of the umap-embedding"
      ],
      "metadata": {
        "id": "BA0sGIBo8vr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.from_scipy_sparse_matrix(to_cluster_umap.graph_)\n"
      ],
      "metadata": {
        "id": "UwotRfvNbUZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfGHvvoR_MtY"
      },
      "outputs": [],
      "source": [
        "#  we run a clustering solution at high resolution, to produce a coherent color scheme.\n",
        "macro_clustering = cylouvain.best_partition(G, resolution = 5)\n",
        "macro_clustering_solution = list(macro_clustering.values())\n",
        "macro_col_len = len(set(macro_clustering_solution))\n",
        "print(macro_col_len)#6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clustering = cylouvain.best_partition(G, resolution = 1.)\n",
        "clustering_solution = list(clustering.values())\n",
        "col_len = len(set(clustering_solution))\n",
        "print(col_len)#25"
      ],
      "metadata": {
        "id": "hgaKlcLUgrAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "macro_cluster_counter = dict(Counter(macro_clustering_solution))\n",
        "\n",
        "macro_clustering_solution_with_noise = []\n",
        "for assignement in macro_clustering_solution:\n",
        "  if macro_cluster_counter[assignement]<2000:\n",
        "    macro_clustering_solution_with_noise.append(np.nan)\n",
        "  else:\n",
        "    macro_clustering_solution_with_noise.append(assignement)\n",
        "macro_clustering_solution_with_noise = pd.factorize(macro_clustering_solution_with_noise, na_sentinel =-1)[0]\n",
        "\n",
        "\n",
        "cluster_counter = dict(Counter(clustering_solution))\n",
        "\n",
        "clustering_solution_with_noise = []\n",
        "for assignement in clustering_solution:\n",
        "  if cluster_counter[assignement]<1000:\n",
        "    clustering_solution_with_noise.append(np.nan)\n",
        "  else:\n",
        "    clustering_solution_with_noise.append(assignement)\n",
        "clustering_solution_with_noise = pd.factorize(clustering_solution_with_noise, na_sentinel =-1)[0]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_tAJjZDdpWf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "combination_counter = dict(Counter(list(zip(macro_clustering_solution_with_noise,clustering_solution_with_noise))))\n",
        "\n",
        "color_combinations={}\n",
        "for this_cluster in np.unique(clustering_solution_with_noise):\n",
        "  in_these_combinations = []\n",
        "  for this_co in combination_counter.keys():\n",
        "    if this_co[1] == this_cluster:\n",
        "      in_these_combinations.append(this_co)\n",
        "  # print(in_these_combinations)\n",
        "  combo= in_these_combinations[np.argmax([combination_counter[x] for x in in_these_combinations])]\n",
        "  color_combinations[combo[1]] = combo[0]\n"
      ],
      "metadata": {
        "id": "KQnPrS0I-McL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(np.unique(macro_clustering_solution_with_noise)))\n",
        "macro_cluster_colors = ['#b3c3b9','#b9e26f','#4e92ba','#ff8c02','#d15a0a','#541836']\n",
        "\n",
        "sns.palplot(macro_cluster_colors)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x6sxc27bDMe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colorsys\n",
        "\n",
        "def hex2rgb(hex_value):\n",
        "    h = hex_value.strip(\"#\") \n",
        "    rgb = tuple(int(h[i:i+2], 16) for i in (0, 2, 4))\n",
        "    return [x/255 for x in rgb]\n",
        "\n",
        "def return_jittered_color(my_color):\n",
        "  hls = colorsys.rgb_to_hls(*hex2rgb(my_color))\n",
        "  jitter = [(np.random.rand()-.5)*.05,\n",
        "            (np.random.rand()-.5)*.2,\n",
        "            (np.random.rand()-.5)*.2]\n",
        "  hls = [abs(x+y)  if abs(x+y)<1 else 1. for x,y in zip(hls,jitter)]\n",
        "  return colorsys.hls_to_rgb(*hls)\n",
        "\n",
        "def return_jittered_color_rgb(my_color):\n",
        "  rgb = hex2rgb(my_color)\n",
        "  jitter = [np.random.rand()*.2,\n",
        "            np.random.rand()*.2,\n",
        "            np.random.rand()*.2]\n",
        "  rgb = [abs(x+y)  if abs(x+y)<1 else 1. for x,y in zip(rgb,jitter)]\n",
        "  return rgb\n",
        "\n",
        "\n",
        "# return_jittered_color('#b3c3b9')\n",
        "micro_colors = [macro_cluster_colors[color_combinations[x] + 1] for x in np.unique(clustering_solution_with_noise)]\n",
        "sns.palplot(micro_colors)\n",
        "micro_colors = [return_jittered_color(x) for x in micro_colors]\n",
        "print(micro_colors)\n",
        "sns.palplot(micro_colors)\n",
        "colors_to_plot = [micro_colors[x+1] for x in clustering_solution_with_noise]"
      ],
      "metadata": {
        "id": "JDrEjepI1Z7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot, colored by source"
      ],
      "metadata": {
        "id": "qcTcDNhl-rs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import palettable\n",
        "origin_colormap = palettable.wesanderson.GrandBudapest4_5.mpl_colormap\n",
        "\n",
        "\n",
        "origin_factors = pd.factorize(full_data['origin'])\n",
        "plot_idx = np.random.permutation(full_data.shape[0])\n",
        "\n",
        "sns.set(font=\"Source Sans Pro\",font_scale=2.1)\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(30,30))\n",
        "scatter = plt.scatter(to_cluster[plot_idx, 0],\n",
        "                      to_cluster[plot_idx, 1],\n",
        "                      # s=[len(x)*4 for x in list(combined_dataset['article_tuple_lists'])],\n",
        "                      c=origin_factors[0][plot_idx],\n",
        "                      cmap=origin_colormap, alpha=1,s=1.5)\n",
        "\n",
        "# plt.scatter(umapped_articles[:, 0], umapped_articles[:, 1],\n",
        "#             s= [len(x)*4 for x in list(combined_dataset['article_tuple_lists'])]\n",
        "#             , c=combined_dataset['origin'][plot_idx].factorize()[0],alpha=0.2)\n",
        "\n",
        "plt.legend(handles=scatter.legend_elements()[0],\n",
        "           labels=['BioRxiv','ArXiv'],markerscale=4)\n",
        "scatter.set_alpha(.2) #set alpha after plotting the legend ;-)\n",
        "\n",
        "\n",
        "plt.axis('equal')\n",
        "plt.savefig('combined_set_ArX_BioRx.png', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j1EUD-iJHSkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot, colored by cluster"
      ],
      "metadata": {
        "id": "enRZeTez-zm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import colorcet as cc\n",
        "\n",
        "sns.set(font=\"Source Sans Pro\",font_scale=2.1)\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(30,30))\n",
        "scatter = plt.scatter(to_cluster[:, 0],\n",
        "                      to_cluster[:, 1],\n",
        "                      c=colors_to_plot, alpha=.2,s=1.5)\n",
        "\n",
        "\n",
        "plt.axis('equal')\n",
        "plt.savefig('combined_set_cluster.png', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lwO4VIkdsT7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract labels for clusters"
      ],
      "metadata": {
        "id": "A4jAPbZVOySC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUak1_J9_Mta"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "article_cluster_labels = []\n",
        "for cluster_num in tqdm(np.unique(clustering_solution_with_noise)):\n",
        "    w = np.where(np.array(clustering_solution_with_noise)==cluster_num)[0]\n",
        "    cluster_vectors = bow[w]\n",
        "    kws = [vec.get_feature_names()[i] for i in np.asarray(cluster_vectors.sum(axis=0)).flatten().argsort()[-12:]][::-1]\n",
        "    article_cluster_labels.append(', '.join(kws))\n",
        "     \n",
        "article_cluster_labels_wrapped = [\"\\n\".join(textwrap.wrap(i,60)) for i in article_cluster_labels]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  !rm cluster_labels.txt\n",
        "except:\n",
        "  pass\n",
        "  \n",
        "for cluster in article_cluster_labels:\n",
        "  print(cluster)\n",
        "  with open('drive/MyDrive/ARXIV_FORMULA_PARSING/cluster_labels.txt', 'a') as f:\n",
        "    f.write(cluster)\n",
        "    f.write('\\n')"
      ],
      "metadata": {
        "id": "lhpZIEu88rYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data['cluster'] = clustering_solution_with_noise\n",
        "full_data['color'] = colors_to_plot\n",
        "\n",
        "\n",
        "\n",
        "dump(full_data, \"drive/MyDrive/ARXIV_FORMULA_PARSING/full_data_rejoined_with_clusters.bz\")"
      ],
      "metadata": {
        "id": "6WNOpaiHT8q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3AzXEyXIP273"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot cluster-labels as base for final graphic"
      ],
      "metadata": {
        "id": "kt3GjVLjP5bE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "0sYVP-Xv_Mtb"
      },
      "outputs": [],
      "source": [
        "\n",
        "sns.set(font=\"Source Sans Pro\",font_scale=2.1)\n",
        "\n",
        "x_dim = 4\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "cluster_vector = list(range(0,len(np.unique(clustering_solution_with_noise))))\n",
        "cluster_centers=[]\n",
        "for small_clust in np.unique(clustering_solution_with_noise):\n",
        "    where_small_clust = np.where(np.array(clustering_solution_with_noise)==small_clust)[0]\n",
        "    cluster_centers.append(np.median(to_cluster[where_small_clust],axis=0))\n",
        "cluster_centers = np.array(cluster_centers)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(40,40))\n",
        "\n",
        "\n",
        "plt.scatter(to_cluster[:, 0], to_cluster[:, 1], s=10, c=colors_to_plot)\n",
        " \n",
        "\n",
        "\n",
        "plot_dimensions = [np.min(to_cluster[:, 0]),np.max(to_cluster[:, 0]),np.min(to_cluster[:, 1]),np.max(to_cluster[:, 1])]\n",
        "print(plot_dimensions)\n",
        "\n",
        "x_label_size = 3\n",
        "y_label_size = 1\n",
        "\n",
        "plot_width = plot_dimensions[1] - plot_dimensions[0]\n",
        "plot_height = plot_dimensions[3] - plot_dimensions[2]\n",
        "\n",
        "margin_size = 400\n",
        "\n",
        "\n",
        "left_angles = [i*(math.pi*2) for i in list(np.linspace(0.35,0.65,num=int(np.ceil(len(cluster_centers) / 2))))] \n",
        "\n",
        "right_angles = [i*(math.pi*2) for i in list(np.linspace(0.85,1.15,num=int(np.floor(len(cluster_centers) / 2))))] \n",
        "\n",
        "\n",
        "angle = left_angles + right_angles\n",
        "print(angle)\n",
        "\n",
        "circle_x = [(math.cos(i)*plot_width/1.2 + (plot_dimensions[0]+plot_width/2))for i in angle]\n",
        "circle_y = [math.sin(i)*plot_height/1.9 + (plot_dimensions[2]+plot_height/2) for i in angle]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from scipy.spatial import distance_matrix\n",
        "\n",
        "euclidean_cost_matrix = distance_matrix(cluster_centers, np.array([circle_x,circle_y]).transpose())\n",
        "row_ind, col_ind = linear_sum_assignment(euclidean_cost_matrix)\n",
        "\n",
        "\n",
        "horizontal_constant = 0.66\n",
        "marker_text_move = 0.5\n",
        "for i in range(len(cluster_centers)):\n",
        "    \n",
        "\n",
        "    plt.scatter(circle_x[col_ind[i]],circle_y[col_ind[i]],s=300,c=np.array([micro_colors[i]]),marker='s')#,cmap=cmap)\n",
        "\n",
        "\n",
        "    if circle_x[col_ind[i]] < plot_dimensions[0]+plot_width/2: #check if left\n",
        "        dist = (cluster_centers[i,0] - circle_x[col_ind[i]]) * horizontal_constant\n",
        "\n",
        "        ax.plot([circle_x[col_ind[i]],circle_x[col_ind[i]]+dist],\n",
        "           [circle_y[col_ind[i]],circle_y[col_ind[i]]],\n",
        "                c='black',linestyle=':',linewidth=3)\n",
        "        \n",
        "        ax.plot([circle_x[col_ind[i]]+dist,cluster_centers[i,0]],\n",
        "           [circle_y[col_ind[i]],cluster_centers[i,1]],\n",
        "                c='black',linestyle=':',linewidth=3)        \n",
        "\n",
        "        \n",
        "        ax.text(circle_x[col_ind[i]]-marker_text_move,circle_y[col_ind[i]], article_cluster_labels_wrapped[i],\n",
        "           verticalalignment='center', horizontalalignment='right',style='italic')\n",
        "        \n",
        "        \n",
        "    else:\n",
        "        \n",
        "        dist = (circle_x[col_ind[i]] - cluster_centers[i,0]) * horizontal_constant\n",
        "\n",
        "        ax.plot([circle_x[col_ind[i]],circle_x[col_ind[i]]-dist],\n",
        "           [circle_y[col_ind[i]],circle_y[col_ind[i]]],\n",
        "                c='black',linestyle=':',linewidth=3)\n",
        "        \n",
        "        ax.plot([circle_x[col_ind[i]]-dist,cluster_centers[i,0]],\n",
        "           [circle_y[col_ind[i]],cluster_centers[i,1]],\n",
        "                c='black',linestyle=':',linewidth=3)     \n",
        "        \n",
        "        \n",
        "        ax.text(circle_x[col_ind[i]]+marker_text_move,circle_y[col_ind[i]], article_cluster_labels_wrapped[i],\n",
        "           verticalalignment='center', horizontalalignment='left',style='italic')\n",
        "plt.axis('off')\n",
        "\n",
        "\n",
        "\n",
        "plt.savefig('labeled_graphic.png', dpi=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot overview graphic with varying settings"
      ],
      "metadata": {
        "id": "hRTIQ8JxQDqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from umap.umap_ import nearest_neighbors\n",
        "\n",
        "\n",
        "\n",
        "n_neighbors = [5, 20,35,50]\n",
        "min_dists = [0, 0.1,0.2,0.3]\n",
        "\n",
        "precomputed_knn_embeddings = np.zeros((4, 4, len(XSVD), 2))\n",
        "\n",
        "_knn = nearest_neighbors(XSVD,\n",
        "                              n_neighbors=np.max(n_neighbors),\n",
        "                              metric=\"cosine\",\n",
        "                              metric_kwds=None,\n",
        "                              angular=False,\n",
        "                              random_state=None,verbose=True\n",
        "                             )\n"
      ],
      "metadata": {
        "id": "v_3-Y9HhI4OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, k in enumerate(n_neighbors):\n",
        "    for j, dist in enumerate(min_dists):\n",
        "        precomputed_knn_embeddings[i, j] = umap.UMAP(n_neighbors=k,\n",
        "                                                      min_dist=dist,\n",
        "                                                      precomputed_knn=_knn,verbose=True\n",
        "                                                      ).fit_transform(XSVD)"
      ],
      "metadata": {
        "id": "6_SVKVz7I4V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from compress_pickle import dump, load\n",
        "fname = \"drive/MyDrive/preprint_2019-2020_datasets/precomputed_knn_embeddings.bz\" \n",
        "dump(precomputed_knn_embeddings, fname)"
      ],
      "metadata": {
        "id": "WML4kysfO59V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(4, 4, figsize=(30,30))\n",
        "\n",
        "for i, ax_row in enumerate(axs):\n",
        "    for j, ax in enumerate(ax_row):\n",
        "        ax.scatter(precomputed_knn_embeddings[i, j, :, 0],\n",
        "                   precomputed_knn_embeddings[i, j, :, 1],\n",
        "                  #  c=labels / 9,\n",
        "                  #  cmap='tab10',\n",
        "                   c=colors_to_plot,\n",
        "                   alpha=0.1,\n",
        "                   s=.8,\n",
        "                   )\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        if i == 0:\n",
        "            ax.set_title(\"min_dist = {}\".format(min_dists[j]), size=15)\n",
        "        if j == 0:\n",
        "            ax.set_ylabel(\"n_neighbors = {}\".format(n_neighbors[i]), size=15)\n",
        "fig.suptitle(\"UMAP embedding of MNIST digits with grid of parameters\", y=0.92, size=20)\n",
        "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
        "\n",
        "plt.savefig('overview_of_textual_umap_runs.png', dpi=300)#, ext='png', bbox_inches=\"tight\")  \n"
      ],
      "metadata": {
        "id": "eUAeSEXfO6Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate progress graphics for illustration\n"
      ],
      "metadata": {
        "id": "dRRmuMpndSaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reducer = umap.UMAP(random_state=20,\n",
        "                    n_components=2,\n",
        "                    n_neighbors=20,\n",
        "                    min_dist=0.1,\n",
        "                    metric='cosine',\n",
        "                    verbose=True,\n",
        "                    low_memory=True,\n",
        "                    n_epochs=0)\n",
        "\n",
        "import palettable\n",
        "\n",
        "init = XSVD[:,0:2]\n",
        "\n",
        "for n_e in [0,1,2,5,10,20,50,100,200]:\n",
        "\n",
        "  reducer = umap.UMAP(random_state=20,\n",
        "                    n_components=2,\n",
        "                    n_neighbors=20,\n",
        "                    min_dist=0.01,\n",
        "                    metric='cosine',\n",
        "                    verbose=True,\n",
        "                    low_memory=True,\n",
        "                    n_epochs=n_e, init = init)\n",
        "  \n",
        "\n",
        "  embedding = reducer.fit_transform(XSVD)\n",
        "\n",
        "  origin_factors = pd.factorize(full_data['origin'])\n",
        "  plot_idx = np.random.permutation(full_data.shape[0])\n",
        "\n",
        "  \n",
        "  origin_colormap = palettable.wesanderson.GrandBudapest4_5.mpl_colormap\n",
        "\n",
        "  plot_idx = np.random.permutation(embedding.shape[0])\n",
        "\n",
        "  sns.set(font=\"Source Sans Pro\",font_scale=2.1)\n",
        "  sns.set_style(\"white\")\n",
        "  fig, ax = plt.subplots(figsize=(30,20))\n",
        "  scatter = plt.scatter(embedding[plot_idx, 0],\n",
        "                        embedding[plot_idx, 1],\n",
        "                        s=1.5,\n",
        "                        c=origin_factors[0][plot_idx],\n",
        "                        cmap=origin_colormap,\n",
        "                         alpha=1)\n",
        "  \n",
        "\n",
        "  plt.legend(handles=scatter.legend_elements()[0], labels=['ArXiv', 'BioRxiv'],markerscale=4)\n",
        "  scatter.set_alpha(0.5) #set alpha after plotting the legend ;-)\n",
        "\n",
        "\n",
        "  plt.axis('equal')\n",
        "  plt.savefig('combined_set_ArX_BioRx_ne_'+str(n_e)+'.png', dpi=300)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ME8MMAP9dFQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "oldHeight": 557.097222,
      "position": {
        "height": "585.917px",
        "left": "1847px",
        "right": "20px",
        "top": "112px",
        "width": "346.867px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "varInspector_section_display": "block",
      "window_display": false
    },
    "colab": {
      "name": "text_layout.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "eaK0C2LR_MtM"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}